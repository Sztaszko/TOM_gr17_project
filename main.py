# -*- coding: utf-8 -*-
"""main2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWnxt_shKHoG0i7QGvvJk7YXAVorCPhp
"""

# Commented out IPython magic to ensure Python compatibility.
! curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
! sudo apt-get install git-lfs
! git lfs install
! git clone https://github.com/neheller/kits19
# %cd kits19/
! python -m starter_code.get_imaging

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
from preprocessing import *
from unet import *


#definiowanie zbiorow
TRAIN_CASES=10
VAL_CASES=5   #validation
TEST_CASES=5



def generate_volume(cid, base_path):
  #base_path wskazuje np na content/train
  #os.path.abspath('') zwraca lokalizacje w kits19
  data_path = Path('kits19/data')

  case_id = "case_{:05d}".format(cid)
  case_path = data_path / case_id
  vol = nib.load(str(case_path / "imaging.nii.gz"))

  vol = vol.get_fdata()

  
  out_path = base_path / "vol"
  check_dir(out_path)

  out_path = out_path / case_id
  check_dir(out_path)

  for i in range(vol.shape[0]):
    fpath = out_path / ("{:05d}.png".format(i))
    case_im ="{:05d}.png".format(i)
    if case_im in os.listdir(out_path):
      print("zdjecie juz jest", case_im)
    else:
      plt.imsave(str(fpath), vol[i], cmap = 'gray')

def generate_segm(cid, base_path):
  #base_path wskazuje np na content/train
  #os.path.abspath('') zwraca lokalizacje w kits19
  data_path = Path('kits19/data')

  case_id = "case_{:05d}".format(cid)
  case_path = data_path / case_id
  segm = nib.load(str(case_path / "segmentation.nii.gz"))

  segm = segm.get_fdata()

  out_path = base_path / "segm"
  check_dir(out_path)

  out_path = out_path/ case_id
  check_dir(out_path)

  for i in range(segm.shape[0]):
    fpath = out_path / ("{:05d}.png".format(i))
    case_im = "{:05d}.png".format(i)
    if case_im in os.listdir(out_path):
      print("zdjecie juz jest", case_im)
    else:
      plt.imsave(str(fpath), segm[i], cmap = 'gray')


#zbior treningowy
train_path=Path('/content/train')
check_dir(train_path)
for i in range(TRAIN_CASES):
  generate_volume(i,train_path)
  generate_segm(i,train_path)

#zbior walidacyjny
val_path=Path('/content/val')
check_dir(val_path)
for i in range(VAL_CASES):
  generate_volume(TRAIN_CASES+i, val_path)
  generate_segm(TRAIN_CASES+i,val_path)

#zbior testowy
test_path=Path('/content/test')
check_dir(test_path)
for i in range(TEST_CASES):
  generate_volume(200+i, test_path)
  generate_segm(200+i, test_path)



#preprocessing zbior√≥w 
train_X=preprocess_vol(train_path/'vol')
train_Y=preprocess_segm(train_path/'segm')

val_X=preprocess_vol(val_path/'vol')
val_Y=preprocess_segm(val_path/'segm')

test_X=preprocess_vol(test_path/'vol')
test_Y=preprocess_segm(test_path/'segm')


nb_train_samples=len(train_X.filenames)
nb_validation_samples=len(val_X.filenames)
nb_test_samples=len(test_X.filenames)

#definiowanie sieci
IMG_WIDTH=256
IMG_HEIGHT=256
IMG_CHANNELS=1

inputs = tf.keras.layers.Input((IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS))
outputs=build_model(inputs)
model = tf.keras.Model(inputs=[inputs], outputs=[outputs])

model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])
model.summary()

#adding checkpoints
checkpointer=tf.keras.callbacks.ModelCheckpoint('/tmp/model1.h5', verbose=1, save_best_only=True)

callbacks=[tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_loss'), 
            tf.keras.callbacks.TensorBoard(log_dir='/tmp/logs')]

train_set=zip(train_X,train_Y)
val_set=zip(val_X,val_Y)



batch_size=32
epochs=25
results=model.fit_generator(
    train_set,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=val_set,
    validation_steps=nb_validation_samples // batch_size,
    callbacks=callbacks)

plt.figure()
plt.plot(model.history.history['dice_coef'])
plt.ylabel('dice_coef')
plt.xlabel('Epochs')
plt.show()